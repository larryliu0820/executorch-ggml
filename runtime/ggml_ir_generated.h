// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_GGMLIR_GGML_IR_H_
#define FLATBUFFERS_GENERATED_GGMLIR_GGML_IR_H_

#include "flatbuffers/flatbuffers.h"

// Ensure the included flatbuffers.h is the same version as when this file was
// generated, otherwise it may not be compatible.
static_assert(FLATBUFFERS_VERSION_MAJOR == 24 &&
              FLATBUFFERS_VERSION_MINOR == 3 &&
              FLATBUFFERS_VERSION_REVISION == 25,
             "Non-compatible flatbuffers version included");

namespace ggml_ir {

struct Tensor;
struct TensorBuilder;

struct GgmlGraph;
struct GgmlGraphBuilder;

enum OpCode : int32_t {
  OpCode_NONE = 0,
  OpCode_ADD = 1,
  OpCode_MUL_MAT = 2,
  OpCode_LEAKY_RELU = 3,
  OpCode_CONV_2D = 4,
  OpCode_CONV_2D_DW = 5,
  OpCode_HARDTANH = 6,
  OpCode_MEAN = 7,
  OpCode_VIEW = 8,
  OpCode_PERMUTE = 9,
  OpCode_MUL = 10,
  OpCode_NEG = 11,
  OpCode_SUB = 12,
  OpCode_MUL_SCALAR = 13,
  OpCode_POW = 14,
  OpCode_COS = 15,
  OpCode_SIN = 16,
  OpCode_BMM = 17,
  OpCode_SIGMOID = 18,
  OpCode_SOFTMAX = 19,
  OpCode_LINEAR = 20,
  OpCode_EMBEDDING = 21,
  OpCode_SILU = 22,
  OpCode_RSQRT = 30,
  OpCode_UNSQUEEZE = 31,
  OpCode_TRANSPOSE = 40,
  OpCode_SLICE = 41,
  OpCode_CAT = 42,
  OpCode_REPEAT_INTERLEAVE = 43,
  OpCode_INDEX = 44,
  OpCode_INDEX_PUT = 45,
  OpCode_REPEAT = 46,
  OpCode_INDEX_MULTI = 47,
  OpCode_CAST = 48,
  OpCode_WHERE = 50,
  OpCode_ARANGE = 51,
  OpCode_FULL = 52,
  OpCode_CUMSUM = 53,
  OpCode_EQ = 54,
  OpCode_NE = 55,
  OpCode_LE = 56,
  OpCode_LT = 57,
  OpCode_GT = 58,
  OpCode_GE = 59,
  OpCode_LLAMA_ATTENTION = 60,
  OpCode_BITWISE_AND = 70,
  OpCode_BITWISE_OR = 71,
  OpCode_LOGICAL_NOT = 72,
  OpCode_ANY = 73,
  OpCode_UPDATE_CACHE = 74,
  OpCode_MIN = OpCode_NONE,
  OpCode_MAX = OpCode_UPDATE_CACHE
};

inline const OpCode (&EnumValuesOpCode())[50] {
  static const OpCode values[] = {
    OpCode_NONE,
    OpCode_ADD,
    OpCode_MUL_MAT,
    OpCode_LEAKY_RELU,
    OpCode_CONV_2D,
    OpCode_CONV_2D_DW,
    OpCode_HARDTANH,
    OpCode_MEAN,
    OpCode_VIEW,
    OpCode_PERMUTE,
    OpCode_MUL,
    OpCode_NEG,
    OpCode_SUB,
    OpCode_MUL_SCALAR,
    OpCode_POW,
    OpCode_COS,
    OpCode_SIN,
    OpCode_BMM,
    OpCode_SIGMOID,
    OpCode_SOFTMAX,
    OpCode_LINEAR,
    OpCode_EMBEDDING,
    OpCode_SILU,
    OpCode_RSQRT,
    OpCode_UNSQUEEZE,
    OpCode_TRANSPOSE,
    OpCode_SLICE,
    OpCode_CAT,
    OpCode_REPEAT_INTERLEAVE,
    OpCode_INDEX,
    OpCode_INDEX_PUT,
    OpCode_REPEAT,
    OpCode_INDEX_MULTI,
    OpCode_CAST,
    OpCode_WHERE,
    OpCode_ARANGE,
    OpCode_FULL,
    OpCode_CUMSUM,
    OpCode_EQ,
    OpCode_NE,
    OpCode_LE,
    OpCode_LT,
    OpCode_GT,
    OpCode_GE,
    OpCode_LLAMA_ATTENTION,
    OpCode_BITWISE_AND,
    OpCode_BITWISE_OR,
    OpCode_LOGICAL_NOT,
    OpCode_ANY,
    OpCode_UPDATE_CACHE
  };
  return values;
}

inline const char * const *EnumNamesOpCode() {
  static const char * const names[76] = {
    "NONE",
    "ADD",
    "MUL_MAT",
    "LEAKY_RELU",
    "CONV_2D",
    "CONV_2D_DW",
    "HARDTANH",
    "MEAN",
    "VIEW",
    "PERMUTE",
    "MUL",
    "NEG",
    "SUB",
    "MUL_SCALAR",
    "POW",
    "COS",
    "SIN",
    "BMM",
    "SIGMOID",
    "SOFTMAX",
    "LINEAR",
    "EMBEDDING",
    "SILU",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "RSQRT",
    "UNSQUEEZE",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "TRANSPOSE",
    "SLICE",
    "CAT",
    "REPEAT_INTERLEAVE",
    "INDEX",
    "INDEX_PUT",
    "REPEAT",
    "INDEX_MULTI",
    "CAST",
    "",
    "WHERE",
    "ARANGE",
    "FULL",
    "CUMSUM",
    "EQ",
    "NE",
    "LE",
    "LT",
    "GT",
    "GE",
    "LLAMA_ATTENTION",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "BITWISE_AND",
    "BITWISE_OR",
    "LOGICAL_NOT",
    "ANY",
    "UPDATE_CACHE",
    nullptr
  };
  return names;
}

inline const char *EnumNameOpCode(OpCode e) {
  if (::flatbuffers::IsOutRange(e, OpCode_NONE, OpCode_UPDATE_CACHE)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesOpCode()[index];
}

enum TensorType : int32_t {
  TensorType_F32 = 0,
  TensorType_F16 = 1,
  TensorType_I64 = 2,
  TensorType_I32 = 3,
  TensorType_BOOL = 4,
  TensorType_BF16 = 5,
  TensorType_MIN = TensorType_F32,
  TensorType_MAX = TensorType_BF16
};

inline const TensorType (&EnumValuesTensorType())[6] {
  static const TensorType values[] = {
    TensorType_F32,
    TensorType_F16,
    TensorType_I64,
    TensorType_I32,
    TensorType_BOOL,
    TensorType_BF16
  };
  return values;
}

inline const char * const *EnumNamesTensorType() {
  static const char * const names[7] = {
    "F32",
    "F16",
    "I64",
    "I32",
    "BOOL",
    "BF16",
    nullptr
  };
  return names;
}

inline const char *EnumNameTensorType(TensorType e) {
  if (::flatbuffers::IsOutRange(e, TensorType_F32, TensorType_BF16)) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesTensorType()[index];
}

struct Tensor FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef TensorBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ID = 4,
    VT_TYPE = 6,
    VT_NE = 8,
    VT_OP = 10,
    VT_SRC_IDS = 12,
    VT_OP_PARAMS = 14,
    VT_DATA_KEY = 16,
    VT_IS_INPUT = 18,
    VT_IS_OUTPUT = 20,
    VT_INPUT_INDEX = 22,
    VT_DYNAMIC_DIMS = 24
  };
  int32_t id() const {
    return GetField<int32_t>(VT_ID, 0);
  }
  ggml_ir::TensorType type() const {
    return static_cast<ggml_ir::TensorType>(GetField<int32_t>(VT_TYPE, 0));
  }
  const ::flatbuffers::Vector<int64_t> *ne() const {
    return GetPointer<const ::flatbuffers::Vector<int64_t> *>(VT_NE);
  }
  ggml_ir::OpCode op() const {
    return static_cast<ggml_ir::OpCode>(GetField<int32_t>(VT_OP, 0));
  }
  const ::flatbuffers::Vector<int32_t> *src_ids() const {
    return GetPointer<const ::flatbuffers::Vector<int32_t> *>(VT_SRC_IDS);
  }
  const ::flatbuffers::Vector<uint8_t> *op_params() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_OP_PARAMS);
  }
  const ::flatbuffers::String *data_key() const {
    return GetPointer<const ::flatbuffers::String *>(VT_DATA_KEY);
  }
  bool is_input() const {
    return GetField<uint8_t>(VT_IS_INPUT, 0) != 0;
  }
  bool is_output() const {
    return GetField<uint8_t>(VT_IS_OUTPUT, 0) != 0;
  }
  int32_t input_index() const {
    return GetField<int32_t>(VT_INPUT_INDEX, -1);
  }
  const ::flatbuffers::Vector<uint8_t> *dynamic_dims() const {
    return GetPointer<const ::flatbuffers::Vector<uint8_t> *>(VT_DYNAMIC_DIMS);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int32_t>(verifier, VT_ID, 4) &&
           VerifyField<int32_t>(verifier, VT_TYPE, 4) &&
           VerifyOffset(verifier, VT_NE) &&
           verifier.VerifyVector(ne()) &&
           VerifyField<int32_t>(verifier, VT_OP, 4) &&
           VerifyOffset(verifier, VT_SRC_IDS) &&
           verifier.VerifyVector(src_ids()) &&
           VerifyOffset(verifier, VT_OP_PARAMS) &&
           verifier.VerifyVector(op_params()) &&
           VerifyOffset(verifier, VT_DATA_KEY) &&
           verifier.VerifyString(data_key()) &&
           VerifyField<uint8_t>(verifier, VT_IS_INPUT, 1) &&
           VerifyField<uint8_t>(verifier, VT_IS_OUTPUT, 1) &&
           VerifyField<int32_t>(verifier, VT_INPUT_INDEX, 4) &&
           VerifyOffset(verifier, VT_DYNAMIC_DIMS) &&
           verifier.VerifyVector(dynamic_dims()) &&
           verifier.EndTable();
  }
};

struct TensorBuilder {
  typedef Tensor Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_id(int32_t id) {
    fbb_.AddElement<int32_t>(Tensor::VT_ID, id, 0);
  }
  void add_type(ggml_ir::TensorType type) {
    fbb_.AddElement<int32_t>(Tensor::VT_TYPE, static_cast<int32_t>(type), 0);
  }
  void add_ne(::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> ne) {
    fbb_.AddOffset(Tensor::VT_NE, ne);
  }
  void add_op(ggml_ir::OpCode op) {
    fbb_.AddElement<int32_t>(Tensor::VT_OP, static_cast<int32_t>(op), 0);
  }
  void add_src_ids(::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> src_ids) {
    fbb_.AddOffset(Tensor::VT_SRC_IDS, src_ids);
  }
  void add_op_params(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> op_params) {
    fbb_.AddOffset(Tensor::VT_OP_PARAMS, op_params);
  }
  void add_data_key(::flatbuffers::Offset<::flatbuffers::String> data_key) {
    fbb_.AddOffset(Tensor::VT_DATA_KEY, data_key);
  }
  void add_is_input(bool is_input) {
    fbb_.AddElement<uint8_t>(Tensor::VT_IS_INPUT, static_cast<uint8_t>(is_input), 0);
  }
  void add_is_output(bool is_output) {
    fbb_.AddElement<uint8_t>(Tensor::VT_IS_OUTPUT, static_cast<uint8_t>(is_output), 0);
  }
  void add_input_index(int32_t input_index) {
    fbb_.AddElement<int32_t>(Tensor::VT_INPUT_INDEX, input_index, -1);
  }
  void add_dynamic_dims(::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> dynamic_dims) {
    fbb_.AddOffset(Tensor::VT_DYNAMIC_DIMS, dynamic_dims);
  }
  explicit TensorBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<Tensor> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<Tensor>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<Tensor> CreateTensor(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int32_t id = 0,
    ggml_ir::TensorType type = ggml_ir::TensorType_F32,
    ::flatbuffers::Offset<::flatbuffers::Vector<int64_t>> ne = 0,
    ggml_ir::OpCode op = ggml_ir::OpCode_NONE,
    ::flatbuffers::Offset<::flatbuffers::Vector<int32_t>> src_ids = 0,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> op_params = 0,
    ::flatbuffers::Offset<::flatbuffers::String> data_key = 0,
    bool is_input = false,
    bool is_output = false,
    int32_t input_index = -1,
    ::flatbuffers::Offset<::flatbuffers::Vector<uint8_t>> dynamic_dims = 0) {
  TensorBuilder builder_(_fbb);
  builder_.add_dynamic_dims(dynamic_dims);
  builder_.add_input_index(input_index);
  builder_.add_data_key(data_key);
  builder_.add_op_params(op_params);
  builder_.add_src_ids(src_ids);
  builder_.add_op(op);
  builder_.add_ne(ne);
  builder_.add_type(type);
  builder_.add_id(id);
  builder_.add_is_output(is_output);
  builder_.add_is_input(is_input);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<Tensor> CreateTensorDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    int32_t id = 0,
    ggml_ir::TensorType type = ggml_ir::TensorType_F32,
    const std::vector<int64_t> *ne = nullptr,
    ggml_ir::OpCode op = ggml_ir::OpCode_NONE,
    const std::vector<int32_t> *src_ids = nullptr,
    const std::vector<uint8_t> *op_params = nullptr,
    const char *data_key = nullptr,
    bool is_input = false,
    bool is_output = false,
    int32_t input_index = -1,
    const std::vector<uint8_t> *dynamic_dims = nullptr) {
  auto ne__ = ne ? _fbb.CreateVector<int64_t>(*ne) : 0;
  auto src_ids__ = src_ids ? _fbb.CreateVector<int32_t>(*src_ids) : 0;
  auto op_params__ = op_params ? _fbb.CreateVector<uint8_t>(*op_params) : 0;
  auto data_key__ = data_key ? _fbb.CreateString(data_key) : 0;
  auto dynamic_dims__ = dynamic_dims ? _fbb.CreateVector<uint8_t>(*dynamic_dims) : 0;
  return ggml_ir::CreateTensor(
      _fbb,
      id,
      type,
      ne__,
      op,
      src_ids__,
      op_params__,
      data_key__,
      is_input,
      is_output,
      input_index,
      dynamic_dims__);
}

struct GgmlGraph FLATBUFFERS_FINAL_CLASS : private ::flatbuffers::Table {
  typedef GgmlGraphBuilder Builder;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_TENSORS = 4,
    VT_N_THREADS = 6
  };
  const ::flatbuffers::Vector<::flatbuffers::Offset<ggml_ir::Tensor>> *tensors() const {
    return GetPointer<const ::flatbuffers::Vector<::flatbuffers::Offset<ggml_ir::Tensor>> *>(VT_TENSORS);
  }
  int32_t n_threads() const {
    return GetField<int32_t>(VT_N_THREADS, 1);
  }
  bool Verify(::flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_TENSORS) &&
           verifier.VerifyVector(tensors()) &&
           verifier.VerifyVectorOfTables(tensors()) &&
           VerifyField<int32_t>(verifier, VT_N_THREADS, 4) &&
           verifier.EndTable();
  }
};

struct GgmlGraphBuilder {
  typedef GgmlGraph Table;
  ::flatbuffers::FlatBufferBuilder &fbb_;
  ::flatbuffers::uoffset_t start_;
  void add_tensors(::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<ggml_ir::Tensor>>> tensors) {
    fbb_.AddOffset(GgmlGraph::VT_TENSORS, tensors);
  }
  void add_n_threads(int32_t n_threads) {
    fbb_.AddElement<int32_t>(GgmlGraph::VT_N_THREADS, n_threads, 1);
  }
  explicit GgmlGraphBuilder(::flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  ::flatbuffers::Offset<GgmlGraph> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = ::flatbuffers::Offset<GgmlGraph>(end);
    return o;
  }
};

inline ::flatbuffers::Offset<GgmlGraph> CreateGgmlGraph(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    ::flatbuffers::Offset<::flatbuffers::Vector<::flatbuffers::Offset<ggml_ir::Tensor>>> tensors = 0,
    int32_t n_threads = 1) {
  GgmlGraphBuilder builder_(_fbb);
  builder_.add_n_threads(n_threads);
  builder_.add_tensors(tensors);
  return builder_.Finish();
}

inline ::flatbuffers::Offset<GgmlGraph> CreateGgmlGraphDirect(
    ::flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<::flatbuffers::Offset<ggml_ir::Tensor>> *tensors = nullptr,
    int32_t n_threads = 1) {
  auto tensors__ = tensors ? _fbb.CreateVector<::flatbuffers::Offset<ggml_ir::Tensor>>(*tensors) : 0;
  return ggml_ir::CreateGgmlGraph(
      _fbb,
      tensors__,
      n_threads);
}

inline const ggml_ir::GgmlGraph *GetGgmlGraph(const void *buf) {
  return ::flatbuffers::GetRoot<ggml_ir::GgmlGraph>(buf);
}

inline const ggml_ir::GgmlGraph *GetSizePrefixedGgmlGraph(const void *buf) {
  return ::flatbuffers::GetSizePrefixedRoot<ggml_ir::GgmlGraph>(buf);
}

inline bool VerifyGgmlGraphBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifyBuffer<ggml_ir::GgmlGraph>(nullptr);
}

inline bool VerifySizePrefixedGgmlGraphBuffer(
    ::flatbuffers::Verifier &verifier) {
  return verifier.VerifySizePrefixedBuffer<ggml_ir::GgmlGraph>(nullptr);
}

inline void FinishGgmlGraphBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<ggml_ir::GgmlGraph> root) {
  fbb.Finish(root);
}

inline void FinishSizePrefixedGgmlGraphBuffer(
    ::flatbuffers::FlatBufferBuilder &fbb,
    ::flatbuffers::Offset<ggml_ir::GgmlGraph> root) {
  fbb.FinishSizePrefixed(root);
}

}  // namespace ggml_ir

#endif  // FLATBUFFERS_GENERATED_GGMLIR_GGML_IR_H_
